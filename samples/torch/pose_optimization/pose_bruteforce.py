# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.
#
# NVIDIA CORPORATION and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.  Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA CORPORATION is strictly prohibited.

import argparse
import os
import pathlib
import sys
import numpy as np
import torch
from PIL import Image
import imageio
import math


UTIL_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'util')
sys.path.append(UTIL_DIR)
import util

import nvdiffrast.torch as dr
from pygltflib import GLTF2

# from Perceptual_loss import PerceptualLoss
# from combined_loss_module import combined_loss, gradient_loss



#ì‚¬ìš©ë²•
# ~.py  --object_name [ê°ì²´ëª… (ex.box)]







#----------------------------------------------------------------------------
# Quaternion math.
#----------------------------------------------------------------------------


# Unit quaternion.
def q_unit():
    return np.asarray([1, 0, 0, 0], np.float32)

# Get a random normalized quaternion.
def q_rnd():
    u, v, w = np.random.uniform(0.0, 1.0, size=[3])
    v *= 2.0 * np.pi
    w *= 2.0 * np.pi
    return np.asarray([(1.0-u)**0.5 * np.sin(v), (1.0-u)**0.5 * np.cos(v), u**0.5 * np.sin(w), u**0.5 * np.cos(w)], np.float32)

# Get a random quaternion from the octahedral symmetric group S_4.
_r2 = 0.5**0.5
_q_S4 = [[ 1.0, 0.0, 0.0, 0.0], [ 0.0, 1.0, 0.0, 0.0], [ 0.0, 0.0, 1.0, 0.0], [ 0.0, 0.0, 0.0, 1.0],
         [-0.5, 0.5, 0.5, 0.5], [-0.5,-0.5,-0.5, 0.5], [ 0.5,-0.5, 0.5, 0.5], [ 0.5, 0.5,-0.5, 0.5],
         [ 0.5, 0.5, 0.5, 0.5], [-0.5, 0.5,-0.5, 0.5], [ 0.5,-0.5,-0.5, 0.5], [-0.5,-0.5, 0.5, 0.5],
         [ _r2,-_r2, 0.0, 0.0], [ _r2, _r2, 0.0, 0.0], [ 0.0, 0.0, _r2, _r2], [ 0.0, 0.0,-_r2, _r2],
         [ 0.0, _r2, _r2, 0.0], [ _r2, 0.0, 0.0,-_r2], [ _r2, 0.0, 0.0, _r2], [ 0.0,-_r2, _r2, 0.0],
         [ _r2, 0.0, _r2, 0.0], [ 0.0, _r2, 0.0, _r2], [ _r2, 0.0,-_r2, 0.0], [ 0.0,-_r2, 0.0, _r2]]
def q_rnd_S4():
    return np.asarray(_q_S4[np.random.randint(24)], np.float32)

# Quaternion slerp.
def q_slerp(p, q, t):
    d = np.dot(p, q)
    if d < 0.0:
        q = -q
        d = -d
    if d > 0.999:
        a = p + t * (q-p)
        return a / np.linalg.norm(a)
    t0 = np.arccos(d)
    tt = t0 * t
    st = np.sin(tt)
    st0 = np.sin(t0)
    s1 = st / st0
    s0 = np.cos(tt) - d*s1
    return s0*p + s1*q

# Quaterion scale (slerp vs. identity quaternion).
def q_scale(q, scl):
    return q_slerp(q_unit(), q, scl)

# Quaternion product.
def q_mul(p, q):
    s1, V1 = p[0], p[1:]
    s2, V2 = q[0], q[1:]
    s = s1*s2 - np.dot(V1, V2)
    V = s1*V2 + s2*V1 + np.cross(V1, V2)
    return np.asarray([s, V[0], V[1], V[2]], np.float32)

# Angular difference between two quaternions in degrees.
def q_angle_deg(p, q):
    p = p.detach().cpu().numpy()
    q = q.detach().cpu().numpy()
    d = np.abs(np.dot(p, q))
    d = min(d, 1.0)
    return np.degrees(2.0 * np.arccos(d))

# Quaternion product
def q_mul_torch(p, q):
    a = p[0]*q[0] - p[1]*q[1] - p[2]*q[2] - p[3]*q[3]
    b = p[0]*q[1] + p[1]*q[0] + p[2]*q[3] - p[3]*q[2]
    c = p[0]*q[2] + p[2]*q[0] + p[3]*q[1] - p[1]*q[3]
    d = p[0]*q[3] + p[3]*q[0] + p[1]*q[2] - p[2]*q[1]
    return torch.stack([a, b, c, d])

# Convert quaternion to 4x4 rotation matrix.
def q_to_mtx(q):
    # r0 = torch.stack([1.0-2.0*q[1]**2 - 2.0*q[2]**2, 2.0*q[0]*q[1] - 2.0*q[2]*q[3], 2.0*q[0]*q[2] + 2.0*q[1]*q[3]])
    # r1 = torch.stack([2.0*q[0]*q[1] + 2.0*q[2]*q[3], 1.0 - 2.0*q[0]**2 - 2.0*q[2]**2, 2.0*q[1]*q[2] - 2.0*q[0]*q[3]])
    # r2 = torch.stack([2.0*q[0]*q[2] - 2.0*q[1]*q[3], 2.0*q[1]*q[2] + 2.0*q[0]*q[3], 1.0 - 2.0*q[0]**2 - 2.0*q[1]**2])
    # rr = torch.transpose(torch.stack([r0, r1, r2]), 1, 0)
    # rr = torch.cat([rr, torch.tensor([[0], [0], [0]], dtype=torch.float32).cuda()], dim=1) # Pad right column.
    # rr = torch.cat([rr, torch.tensor([[0, 0, 0, 1]], dtype=torch.float32).cuda()], dim=0)  # Pad bottom row.
    # return rr
    w, x, y, z = q
    r00 = 1 - 2*y*y - 2*z*z
    r01 = 2*x*y - 2*z*w
    r02 = 2*x*z + 2*y*w
    r10 = 2*x*y + 2*z*w
    r11 = 1 - 2*x*x - 2*z*z
    r12 = 2*y*z - 2*x*w
    r20 = 2*x*z - 2*y*w
    r21 = 2*y*z + 2*x*w
    r22 = 1 - 2*x*x - 2*y*y
    rot = torch.tensor([
        [r00, r01, r02, 0],
        [r10, r11, r12, 0],
        [r20, r21, r22, 0],
        [  0,   0,   0, 1]
    ], dtype=torch.float32).cuda()
    return rot

# ê°ë„ -> ì¿¼í„°ë‹ˆì•ˆ ë³€í™˜
def euler_xyz_to_quaternion(rx, ry, rz):
    cx, cy, cz = np.cos([rx/2, ry/2, rz/2])
    sx, sy, sz = np.sin([rx/2, ry/2, rz/2])
    qw = cx*cy*cz + sx*sy*sz
    qx = sx*cy*cz - cx*sy*sz
    qy = cx*sy*cz + sx*cy*sz
    qz = cx*cy*sz - sx*sy*cz
    return np.array([qw, qx, qy, qz], dtype=np.float32)

#----------------------------------------------------------------------------
# Helpers.
# ì¹´ë©”ë¼ í•´ìƒë„ ë° ê°œë³„ í”½ì…€ í¬ê¸° ì •ë³´ë¡œ ì„¼ì„œ width, height ì •ë³´ ì¶”ë¡ í•˜ëŠ” ì½”ë“œ í•„ìš” (ì¶”í›„ ì¹´ë©”ë¼ ì •ë³´ë¡œë¶€í„° ì •ë³´ë¥¼ ë°›ëŠ”ë‹¤ë©´)

# ì¹´ë©”ë¼ì— ë”°ë¥¸ fov ì¶”ì • ë¡œì§
def compute_fov_y(focal_length_equiv_mm, sensor_width_mm, sensor_height_mm):
    """
    ì„¼ì„œ í¬ê¸°ì™€ ë“±ê°€ ì´ˆì  ê±°ë¦¬ë¡œë¶€í„° ì‹¤ì œ ì´ˆì  ê±°ë¦¬ ë° ìˆ˜ì§ FOV ê³„ì‚°

    Args:
        sensor_width_mm (float): ì„¼ì„œ ê°€ë¡œ ê¸¸ì´ (mm)
        sensor_height_mm (float): ì„¼ì„œ ì„¸ë¡œ ê¸¸ì´ (mm)
        focal_length_equiv_mm (float): 35mm ë“±ê°€ ì´ˆì  ê±°ë¦¬ (mm)

    Returns:
        dict: {
            'sensor_diagonal': ì„¼ì„œ ëŒ€ê°ì„  ê¸¸ì´ (mm),
            'crop_factor': í¬ë¡­ íŒ©í„°,
            'focal_length_actual': ì‹¤ì œ ì´ˆì  ê±°ë¦¬ (mm),
            'fov_y_deg': ìˆ˜ì§ ì‹œì•¼ê° (degree)
        }
    """
    # 1. ì„¼ì„œ ëŒ€ê°ì„ 
    sensor_diagonal = math.sqrt(sensor_width_mm**2 + sensor_height_mm**2)

    # 2. í¬ë¡­ íŒ©í„° (ê¸°ì¤€ ëŒ€ê°ì„  = 43.3mm)
    crop_factor = 43.3 / sensor_diagonal

    # 3. ì‹¤ì œ ì´ˆì  ê±°ë¦¬
    focal_length_actual = focal_length_equiv_mm / crop_factor

    # 4. ìˆ˜ì§ FOV ê³„ì‚°
    fov_y_rad = 2 * math.atan(sensor_height_mm / (2 * focal_length_actual))
    fov_y_deg = math.degrees(fov_y_rad)

    return fov_y_deg

def transform_pos(mtx, pos):
    t_mtx = torch.from_numpy(mtx).cuda() if isinstance(mtx, np.ndarray) else mtx
    posw = torch.cat([pos, torch.ones([pos.shape[0], 1]).cuda()], axis=1)
    return torch.matmul(posw, t_mtx.t())[None, ...]

def render(glctx, mtx, pos, pos_idx, uv, uv_idx, tex, resolution, enable_mip, max_mip_level):
    pos_clip = transform_pos(mtx, pos)
    rast_out, rast_out_db = dr.rasterize(glctx, pos_clip, pos_idx, resolution=[resolution, resolution])

    if enable_mip:
        texc, texd = dr.interpolate(uv[None, ...], rast_out, uv_idx, rast_db=rast_out_db, diff_attrs='all')
        color = dr.texture(tex[None, ...], texc, texd, filter_mode='linear-mipmap-linear', max_mip_level=max_mip_level)
    else:
        texc, _ = dr.interpolate(uv[None, ...], rast_out, uv_idx)
        color = dr.texture(tex[None, ...], texc, filter_mode='linear')

    color = color * torch.clamp(rast_out[..., -1:], 0, 1) # Mask out background.
    return color

def make_grid(arr, ncols=2):
    n, height, width, nc = arr.shape
    nrows = n//ncols
    assert n == nrows*ncols
    return arr.reshape(nrows, ncols, height, width, nc).swapaxes(1,2).reshape(height*nrows, width*ncols, nc)

# glb íŒŒì¼ì— ë”°ë¥¸ ì¶• ë³´ì • ì²˜ë¦¬
def apply_axis_fix(pos, glb_path):
    from pygltflib import GLTF2

    gltf = GLTF2().load(glb_path)
    generator = gltf.asset.generator.lower() if gltf.asset and gltf.asset.generator else ""
    rotation_quat = None
    for node in gltf.nodes:
        if node.rotation:
            rotation_quat = node.rotation
            break

    if "trimesh" in generator:
        x = pos[:, 0].copy()
        y = pos[:, 1].copy()
        z = pos[:, 2].copy()
        pos[:, 0] = x
        pos[:, 1] = -y
        pos[:, 2] = z
        # pass
    elif "blender" in generator:
        x = pos[:, 0].copy()
        y = pos[:, 1].copy()
        z = pos[:, 2].copy()
        pos[:, 0] = x
        pos[:, 1] = z
        pos[:, 2] = y
        # pass
    else:
        # ê¸°íƒ€ íˆ´: ì¶”ê°€ì ì¸ ë¶„ê¸°ê°€ í•„ìš”í•  ìˆ˜ ìˆìŒ (ì—¬ê¸°ì„œëŠ” pass)
        pass

    return pos

#----------------------------------------------------------------------------



def fit_earth(object_name,
              max_iter          = 20000,
              log_interval      = 10,
              display_interval  = None,
              display_res       = 512,
              enable_mip        = True,
              res               = 512,
              ref_res           = 2048,  # Dropped from 4096 to 2048 to allow using the Cuda rasterizer.
              lr_base           = 1e-2,
              lr_ramp          = 0.1,
              nr_base          = 1.0,        # ë…¸ì´ì¦ˆ ê´€ë ¨ íŒŒë¼ë¯¸í„° ì¶”ê°€
              nr_falloff       = 1e-4,       # ë…¸ì´ì¦ˆ ê´€ë ¨ íŒŒë¼ë¯¸í„° ì¶”ê°€
              grad_phase_start = 0.5,        # ê·¸ë˜ë””ì–¸íŠ¸ í˜ì´ì¦ˆ ì‹œì‘ ì‹œì 
              out_dir           = None,
              log_fn            = None,
              texsave_interval  = None,
              texsave_fn        = None,
              imgsave_interval  = None,
              imgsave_fn        = None,
              use_opengl        = False):

    log_file = None
    base_dir = pathlib.Path(__file__).absolute().parents[3]
    
    # out_dir ê´€ë ¨ íŒŒë¼ë¯¸í„° ì „ë¶€ ì£¼ì„ì²˜ë¦¬.
    # if out_dir:
    #     os.makedirs(out_dir, exist_ok=True)
    #     if log_fn:
    #         log_file = open(os.path.join(out_dir, log_fn), 'wt')
    # else:
    #     imgsave_interval, texsave_interval = None, None

# ë°ì´í„° ë””ë ‰í† ë¦¬ ì„¤ì •
    datadir = os.path.join(base_dir, "samples", "data", "npy_data", object_name)
    output_dir = os.path.join(base_dir, "samples", "torch", "output", object_name)
    os.makedirs(output_dir, exist_ok=True)

# obj íŒŒì¼ load ë¶€ë¶„
    # ë°ì´í„° ë¡œë”©
    pos_idx = np.load(os.path.join(datadir, "pos_idx.npy"))
    pos     = np.load(os.path.join(datadir, "pos.npy"))
    #íŒŒì¼ ì„±ê²©ì— ë§ê²Œ ì¶• ë³´ì •. (íŒŒì¼ì´ ì–´ë–¤ í”„ë¡œê·¸ë¨ì„ í†µí•´ì„œ npyë¡œ ë³€í™˜ë˜ì—ˆëŠ”ì§€ëŠ” util\inspect_glb_metadata í™œìš©ìš©)
    pos     = apply_axis_fix(pos, os.path.join(base_dir, "samples", "reference_data","glb_data", f"{object_name}.glb"))
    uv_idx  = np.load(os.path.join(datadir, "uv_idx.npy"))
    uv      = np.load(os.path.join(datadir, "uv.npy"))

    tex_img = Image.open(os.path.join(datadir, "tex.png")).convert("RGB")
    tex     = np.asarray(tex_img).astype(np.float32) / 255.0
    tex_o   = tex.copy()
    max_mip_level = 9

    print("Mesh has %d triangles and %d vertices." % (pos_idx.shape[0], pos.shape[0]))

    # Ref ì´ë¯¸ì§€ ë¡œë“œ
    img_dir = os.path.join(base_dir, "samples", "reference_data","png_seg")
    ref_img_path = os.path.join(img_dir, f"{object_name}_seg.png")
    ref_img = Image.open(ref_img_path).convert("RGB").resize((ref_res, ref_res), Image.LANCZOS)
    ref_img_np = np.asarray(ref_img).astype(np.float32) / 255.0
    img_tensor = torch.from_numpy(ref_img_np).cuda().unsqueeze(0)

    #output_dirì— ì–´ë–¤ refì™€ lossê°€ ê³„ì‚°ë˜ëŠ”ì§€ ì§ê´€ì ìœ¼ë¡œ ë³´ê¸° ìœ„í•¨.
    Image.fromarray((ref_img_np * 255).astype(np.uint8)).save(os.path.join(output_dir, f"{object_name}_ref.png"))


# ë¡œë“œí•œ ë°ì´í„° í…ì„œ ìƒì„±
    # Some input geometry contains vertex positions in (N, 4) (with v[:,3]==1).  Drop
    # the last column in that case.
    if pos.shape[1] == 4: pos = pos[:, 0:3]

    # Create position/triangle index tensors
    pos_idx = torch.from_numpy(pos_idx.astype(np.int32)).cuda()
    vtx_pos = torch.from_numpy(pos.astype(np.float32)).cuda()
    uv_idx  = torch.from_numpy(uv_idx.astype(np.int32)).cuda()
    vtx_uv  = torch.from_numpy(uv.astype(np.float32)).cuda()

    tex     = torch.from_numpy(tex.astype(np.float32)).cuda()
    # tex_opt = torch.full(tex.shape, 0.2, device='cuda', requires_grad=True)
    tex_opt = torch.from_numpy(tex_o.astype(np.float32)).cuda().requires_grad_()
    glctx = dr.RasterizeGLContext() if use_opengl else dr.RasterizeCudaContext()


# ì¿¼í„°ë‹ˆì–¸ í…ì„œ ìƒì„±
    pose_target = torch.tensor(q_rnd(), device='cuda')
    # pose_init   = q_rnd()
    # pose_opt    = torch.tensor(pose_init / np.sum(pose_init**2)**0.5, dtype=torch.float32, device='cuda', requires_grad=True)


# best pose ë¡œì§
    # fov ê´€ë ¨ ë¡œì§ì§
    # # fov_deg = 120.0
    # # fov_rad = math.radians(fov_deg)
    # # compute_fov_y : ë“±ê°€ ì´ˆì ê±°ë¦¬(ì´ë¯¸ì§€ì— ìˆëŠ” ë©”íƒ€ë°ì´í„°) , ì„¼ì„œ width, ì„¼ì„œ hegith
    # # ì•„ì´í° 13pro ì¹´ë©”ë¼ spec
    # #                       ì™€ì´ë“œ ì¹´ë©”ë¼ : 7.66 , 5.75
    # #       ìš¸íŠ¸ë¼ ì™€ì´ë“œ ì¹´ë©”ë¼,ë§ì› ì¹´ë©”ë¼ : 4.03, 3.02
    # fov_rad = math.radians(compute_fov_y(26,7.66,5.75))
    # # ì¢…íš¡ë¹„ (ë Œë”ë§ í•´ìƒë„ ê¸°ì¤€): ì˜ˆì‹œë¡œ 1.0 (ì •ì‚¬ê°í˜• ref ì´ë¯¸ì§€ì— ë§ì¶¤)
    # aspect_ratio = 1.0
    # # Projection matrixì˜ xê°’ ê³„ì‚°
    # x = math.tan(fov_rad / 2.0)
    # # util.projection() í˜¸ì¶œ
    # print(x)

    # proj = util.projection(x=x, n=1.0, f=50.0)
    proj = util.projection(x=0.4, n=1.0, f=200.0)
    # mvp = torch.tensor(np.matmul(proj, util.translate(0, 0, -3.5)).astype(np.float32), device='cuda')

    def silhouette_loss(img1, img2, threshold=0.1):
        """
        ì‹¤ë£¨ì—£ ë¡œìŠ¤ ê³„ì‚° í•¨ìˆ˜
        Args:
            img1, img2: (B, H, W, 3) í˜•íƒœì˜ ì´ë¯¸ì§€ í…ì„œ
            threshold: ë°°ê²½ê³¼ ê°ì²´ë¥¼ êµ¬ë¶„í•˜ëŠ” ì„ê³„ê°’
        Returns:
            ì‹¤ë£¨ì—£ ë¡œìŠ¤ ê°’
        """
        # ì´ë¯¸ì§€ë¥¼ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
        gray1 = 0.299 * img1[..., 0] + 0.587 * img1[..., 1] + 0.114 * img1[..., 2]
        gray2 = 0.299 * img2[..., 0] + 0.587 * img2[..., 1] + 0.114 * img2[..., 2]
        
        # ë§ˆìŠ¤í¬ ìƒì„± (thresholdë³´ë‹¤ í° ê°’ì€ 1, ì‘ì€ ê°’ì€ 0)
        mask1 = (gray1 > threshold).float()
        mask2 = (gray2 > threshold).float()
        
        # ì‹¤ë£¨ì—£ ë¡œìŠ¤ ê³„ì‚° (ë§ˆìŠ¤í¬ ê°„ì˜ ì°¨ì´)
        loss = torch.mean((mask1 - mask2) ** 2)
        return loss

    def coarse_y_search_with_xz_noise(render_fn,
                                    loss_fn,
                                    vtx_pos,
                                    pos_idx,
                                    vtx_uv,
                                    uv_idx,
                                    tex_opt,
                                    img_tensor,
                                    # mvp,
                                    glctx,
                                    res,
                                    enable_mip,
                                    max_mip_level,
                                    object_name,
                                    display_interval=1,
                                    display_res=512,
                                    search_iters=10
                                    ):


        best_loss = float('inf')
        best_pose = None
        pose_log_initialized = False

        color_ref = img_tensor
        while color_ref.shape[1] > res:
            color_ref = util.bilinear_downsample(color_ref)

        # í•œ ë²ˆë§Œ ì €ì¥ (ì›í•˜ë©´)
        color_ref_img = (color_ref[0].cpu().numpy() * 255.0).clip(0, 255).astype(np.uint8)
        Image.fromarray(color_ref_img).save(os.path.join(output_dir, 'color_ref_downsampled.png'))

        for y_deg in range(-10,10,1):
            for roll in range(-3,4,1):
                for pitch in range(-10, 10,1):
                    for translate_z in np.arange(1.8, 2.1, 0.05):
                        for translate_y in np.arange(-0.03, 0.04, 0.01):
                            for translate_x in np.arange(-0.03, 0.04, 0.01):

                                #ê³„ì‚°
                                ry = np.radians(y_deg)
                                rz = np.radians(roll)
                                rx = np.radians(pitch)
                                q_xyz = euler_xyz_to_quaternion(rx, ry, rz)

                                q_combined = q_xyz
                                q_combined /= np.linalg.norm(q_combined)
                                q_tensor = torch.tensor(q_combined, device='cuda', dtype=torch.float32)

                                translate_matrix = torch.eye(4, dtype=torch.float32, device='cuda')
                                translate_matrix[2, 3] = -3.5 + translate_z  # zì¶• ì´ë™
                                translate_matrix[1, 3] = translate_y  # yì¶• ì´ë™
                                translate_matrix[0, 3] = translate_x # xì¶• ì´ë™
                                mvp = torch.matmul(torch.tensor(proj, dtype=torch.float32, device='cuda'), translate_matrix)
                                mtx_total = torch.matmul(mvp, q_to_mtx(q_tensor))


                                color_opt = render_fn(glctx, mtx_total, vtx_pos, pos_idx, vtx_uv, uv_idx,
                                                    tex_opt, res, enable_mip, max_mip_level)
                                

                                # ì†ì‹¤ ê³„ì‚°
                                # loss_fn = PerceptualLoss().cuda()
                                # loss = loss_fn(color_opt, color_ref)
                                # loss = combined_loss(color_opt, color_ref, mode='ssim')   # L2 + SSIM
                                # ë˜ëŠ”
                                # loss = combined_loss(color_opt, color_ref, mode='grad')   # L2 + Gradient
                                diff = (color_opt - color_ref) ** 2
                                diff = torch.tanh(5.0 * torch.max(diff, dim=-1)[0])
                                l2_loss = torch.mean(diff)
                                threshold = 0.01
                                sil_loss = silhouette_loss(color_opt, color_ref, threshold)
                                loss = 0.3 * l2_loss + 0.7 * sil_loss
                                # loss = sil_loss



                                # ìµœì ì˜ í¬ì¦ˆ ì €ì¥
                                loss_val = float(loss)
                                if (loss_val < best_loss) and (loss_val > 0.0):
                                    best_loss = loss_val
                                    best_pose = q_tensor.detach().clone()
                                    # save file
                                    with torch.no_grad():
                                        # Convert tensors to numpy arrays and save
                                        color_opt_np = color_opt[0].cpu().numpy()
                                        # Save color_opt (rendered image)
                                        color_opt_img = (color_opt_np * 255.0).clip(0, 255).astype(np.uint8)
                                        Image.fromarray(color_opt_img).save(os.path.join(output_dir, 
                                        f'loss_{loss_val:.6f}_color_opt_ydeg_{y_deg:03d}_pitch_{pitch:03d}_roll_{roll:03d}_z_{translate_z:.1f}_y_{translate_y:.2f}_x_{translate_x:.2f}.png'))
                                        # Prepare pose log path
                                        pose_log_path = os.path.join(output_dir, f"best_poses_{object_name}.txt")

                                        # Write pose line (overwrite on first write, then append)
                                        mode = "w" if not pose_log_initialized else "a"
                                        with open(pose_log_path, mode) as f:
                                            pose_np = best_pose.cpu().numpy()
                                            pose_str = " ".join([f"{v:.6f}" for v in pose_np])
                                            f.write(f"y_deg={y_deg}, pitch={pitch}, roll={roll}, z={translate_z:.1f},y={translate_y:.1f},x={translate_x:.1f}, loss={loss_val:.6f}, pose=[{pose_str}]\n")

                                        pose_log_initialized = True  # ì´í›„ë¶€í„°ëŠ” append

                                # Print/save log.
                                if 1:
                                    s = ("loss=%f,loss_best=%f,best_pose=%s,y=%2f,p=%2f,r=%2f,z=%2f,y=%2f,x=%2f" % 
                                        (loss_val, best_loss, best_pose.cpu().numpy(), y_deg, pitch, roll, translate_z, translate_y, translate_x))
                                    print(s)
                                    if log_file:
                                        log_file.write(s + "\n")
                                        
                                #ğŸ” Display image every N degrees
                                if 0:
                                    with torch.no_grad():
                                        # # 1. l2 loss êµ¬í•  ë•Œ ì‚¬ìš©í•œ ë‘ ì´ë¯¸ì§€ ì¶œë ¥
                                        # color_ref = img_tensor
                                        # while color_ref.shape[1] > color_opt.shape[1]:
                                        #     color_ref = util.bilinear_downsample(color_ref)

                                        # ref_np = color_ref[0].cpu().numpy()
                                        # opt_np = color_opt[0].cpu().numpy()
                                        
                                        # scl = display_res // opt_np.shape[0]
                                        # ref_img = np.repeat(np.repeat(ref_np, scl, axis=0), scl, axis=1)
                                        # opt_img = np.repeat(np.repeat(opt_np, scl, axis=0), scl, axis=1)

                                        # result_image = np.concatenate([opt_img, ref_img], axis=1)
                                        # util.display_image(result_image, size=display_res, title=f"Y={y_deg}Â°")

                                        # 2. ì‹¤ë£¨ì—£ loss êµ¬í•  ë•Œ ì‚¬ìš©í•œ ë‘ ì´ë¯¸ì§€ ì¶œë ¥
                                        # ì‹¤ë£¨ì—£ ë§ˆìŠ¤í¬ ê³„ì‚°
                                        gray1 = 0.299 * color_opt[0, ..., 0] + 0.587 * color_opt[0, ..., 1] + 0.114 * color_opt[0, ..., 2]
                                        gray2 = 0.299 * color_ref[0, ..., 0] + 0.587 * color_ref[0, ..., 1] + 0.114 * color_ref[0, ..., 2]
                                        
                                        mask1 = (gray1 > threshold).float()
                                        mask2 = (gray2 > threshold).float()

                                        # ë§ˆìŠ¤í¬ë¥¼ ì‹œê°í™”í•˜ê¸° ìœ„í•´ 3ì±„ë„ë¡œ ë³€í™˜
                                        mask1_vis = torch.stack([mask1, mask1, mask1], dim=-1).cpu().numpy()
                                        mask2_vis = torch.stack([mask2, mask2, mask2], dim=-1).cpu().numpy()

                                        # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
                                        scl = display_res // mask1_vis.shape[0]
                                        mask1_img = np.repeat(np.repeat(mask1_vis, scl, axis=0), scl, axis=1)
                                        mask2_img = np.repeat(np.repeat(mask2_vis, scl, axis=0), scl, axis=1)

                                        # ë§ˆìŠ¤í¬ ì´ë¯¸ì§€ ì—°ê²°
                                        result_image = np.concatenate([mask1_img, mask2_img], axis=1)
                                        util.display_image(result_image, size=display_res, title=f"Y={y_deg}Â° - Silhouette Masks")

        return best_pose, best_loss
    # ì œí•œëœ ê°ë„ë¡ ì¿¼í„°ë‹ˆì–¸ ê¸°ë°˜ í¬ì¦ˆ ìµœì í™” ì„¤ì •
    pose_opt, loss_best = coarse_y_search_with_xz_noise(
        render_fn=render,
        loss_fn=lambda out, ref: torch.mean(torch.tanh(5.0 * torch.max((out - ref)**2, dim=-1)[0])),
        vtx_pos=vtx_pos,
        pos_idx=pos_idx,
        vtx_uv=vtx_uv,
        uv_idx=uv_idx,
        tex_opt=tex_opt,
        img_tensor=img_tensor,
        # mvp=mvp,
        glctx=glctx,
        res=res,
        enable_mip=enable_mip,
        max_mip_level=max_mip_level,
        object_name = object_name,
        display_interval=30,
        display_res=512,
        search_iters=10
    )





# ìµœì í™” ê´€ë ¨ ì„¤ì •ì •
#     pose_opt.requires_grad_()
#     # Add translate_z as optimization parameter
#     translate_z = torch.tensor(0.0, device='cuda', requires_grad=True)
#     pose_best   = pose_opt.detach().clone()

# # ìµœì í™” ì½”ë“œ
#     # Adam optimizer for texture with a learning rate ramp.
#     # optimizer    = torch.optim.Adam([tex_opt], lr=lr_base)
#     optimizer = torch.optim.Adam([pose_opt], betas=(0.9, 0.999), lr=lr_base)
#     scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda x: lr_ramp**(float(x)/float(max_iter)))

#     # Create output directory
#     output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "output")
#     os.makedirs(output_dir, exist_ok=True)

#     # Render.
    
#     texloss_avg = []
#     for it in range(max_iter + 1):
#         # í•™ìŠµë¥  ì„¤ì •
#         itf = 1.0 * it / max_iter
#         nr = nr_base * nr_falloff**itf
#         # lr = lr_base * lr_ramp**itf
#         # lr = max(lr_base * lr_ramp**(it / max_iter), 1e-3)
#         lr_base = 1e-1
#         lr_ramp = 0.2       # 10%ë¡œ ì ì  ê°ì†Œ
#         lr_min  = 1e-4      # ìµœì†Œ í•™ìŠµë¥ 

#         # í˜„ì¬ iteration ë¹„ìœ¨
#         itf = it / max_iter

#         # ìŠ¤ì¼€ì¤„ ì ìš©
#         lr = max(lr_base * (lr_ramp ** itf), lr_min)


#         for param_group in optimizer.param_groups:
#             param_group['lr'] = lr

        
#         # ë…¸ì´ì¦ˆ ì¶”ê°€
#         # if itf >= grad_phase_start:
#         #     noise = q_unit()
        
#         noise = q_unit()

#         # # ë Œë”ë§ í–‰ë ¬ ê³„ì‚° - ì›ë˜ ì½”ë“œ ë¶€ë¶„ì´ë‚˜, best pose ë¡œì§ì„ ìœ„í•´ ìœ„ë¡œ ì´ë™ë™
#         # proj = util.projection(x=0.4, n=1.0, f=200.0)
#         # mvp = torch.tensor(np.matmul(proj, util.translate(0, 0, -3.5)).astype(np.float32), device='cuda')


#         # ëª©í‘œ ì´ë¯¸ì§€ì™€ ìµœì í™”ëœ ì´ë¯¸ì§€ ë Œë”ë§
#         color = img_tensor
#         pose_total_opt = q_mul_torch(pose_opt, noise)

#         flip_x = torch.diag(torch.tensor([-1, 1, 1, 1], dtype=torch.float32, device='cuda'))
#         mtx_total_opt = torch.matmul(mvp, torch.matmul(flip_x, q_to_mtx(pose_total_opt)))
#         # mtx_total_opt = torch.matmul(mvp, q_to_mtx(pose_total_opt))
#         color_opt = render(glctx, mtx_total_opt, vtx_pos, pos_idx, vtx_uv, uv_idx, tex_opt, res, enable_mip, max_mip_level)

#         # Reduce the reference to correct size.
#         while color.shape[1] > res:
#             color = util.bilinear_downsample(color)


#         # loss_fn = PerceptualLoss().cuda()
#         # ì†ì‹¤ ê³„ì‚°
#         # loss = loss_fn(color_opt, color)
#         # loss = combined_loss(color_opt, color, mode='ssim')   # L2 + SSIM
#         # ë˜ëŠ”
#         # loss = combined_loss(color_opt, color, mode='grad')   # L2 + Gradient

#         # ì†ì‹¤ ê³„ì‚° ë° ìµœì í™”
#         diff = (color_opt - color)**2 # L2 norm.
#         diff = torch.tanh(5.0 * torch.max(diff, dim=-1)[0])
#         loss = torch.mean(diff)

#         loss_val = float(loss)

#         # # ìµœì ì˜ í¬ì¦ˆ ì €ì¥
#         # if (loss_val < loss_best) and (loss_val > 0.0):
#         #     pose_best = pose_total_opt.detach().clone()
#         #     loss_best = loss_val
#         #     if itf < grad_phase_start:
#         #         with torch.no_grad(): pose_opt[:] = pose_best

#         # Best pose ì €ì¥ or rollback
#         if loss_val < loss_best:
#             loss_best = loss_val
#             pose_best = pose_opt.detach().clone()
#         else:
#             with torch.no_grad():
#                 pose_opt[:] = pose_best  # rollback

#         # ê·¸ë˜ë””ì–¸íŠ¸ ë‹¨ê³„
#         # if itf >= grad_phase_start:
#         optimizer.zero_grad()
#         loss.backward()
#         optimizer.step()

#         # ì¿¼í„°ë‹ˆì–¸ ì •ê·œí™”
#         with torch.no_grad():
#             pose_opt /= torch.sum(pose_opt**2)**0.5

#         # Print/save log.
#         if log_interval and (it % log_interval == 0):
#             err = q_angle_deg(pose_opt, pose_target)
#             ebest = q_angle_deg(pose_best, pose_target)
#             s = "iter=%d,err=%f,err_best=%f,loss=%f,loss_best=%f,lr=%f,nr=%f" % (it, err, ebest, loss_val, loss_best, lr, nr)
#             print(s)
#             if log_file:
#                 log_file.write(s + "\n")

#         # Show/save image.
#         display_image = display_interval and (it % display_interval == 0)
#         save_image = imgsave_interval and (it % imgsave_interval == 0)
#         save_texture = texsave_interval and (it % texsave_interval) == 0

#         if display_image or save_image:

#             with torch.no_grad():

#                 img_b = color[0].cpu().numpy()
#                 img_o = color_opt[0].cpu().numpy()

#                 scl = display_res // img_o.shape[0]
#                 img_b = np.repeat(np.repeat(img_b, scl, axis=0), scl, axis=1)
#                 img_o = np.repeat(np.repeat(img_o, scl, axis=0), scl, axis=1)
#                 result_image = make_grid(np.stack([img_o, img_b]))

#                 if display_image:
#                     util.display_image(result_image, size=display_res, title='%d / %d' % (it, max_iter))
#                 if save_image:
#                     util.save_image(out_dir + '/' + (imgsave_fn % it), result_image)

#                 if save_texture:
#                     texture = tex_opt.cpu().numpy()[::-1]
#                     util.save_image(out_dir + '/' + (texsave_fn % it), texture)

    # save file
    # ì €ì¥í•  í…ì„œ ì´ë¦„ê³¼ ë³€ìˆ˜ ë§¤í•‘
    to_save = {
        "pos_idx": pos_idx,
        "pos":     vtx_pos,
        "uv_idx":  uv_idx,
        "uv":      vtx_uv,
        "tex":     tex_opt,      # ìµœì í™”ëœ í…ìŠ¤ì²˜
    }
    for name, tensor in to_save.items():
        arr = tensor.detach().cpu().numpy()
        if name == "tex":
            # arr shape: (H, W, 3), ê°’ ë²”ìœ„ [0,1] ì¸ float32
            img_arr = (arr * 255.0).clip(0, 255).astype(np.uint8)
            img = Image.fromarray(img_arr)
            out_path = os.path.join(datadir, f"{name}_o.png")
            img.save(out_path)
        else:
            out_path = os.path.join(datadir, f"{name}_o.npy")
            np.save(out_path, arr)
        print(f"Saved {out_path}")
    
    # Done.
    if log_file:
        log_file.close()

#----------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(description='Earth texture fitting example')
    parser.add_argument('--opengl', help='enable OpenGL rendering', action='store_true', default=False)
    parser.add_argument('--outdir', help='specify output directory', default='')
    parser.add_argument('--mip', help='enable mipmapping', action='store_true', default=False)
    parser.add_argument('--display-interval', type=int, default=0)
    parser.add_argument('--max-iter', type=int, default=10000)
    parser.add_argument("--object_name", type=str, required=True, help="Subdirectory under obj_data (e.g., book, chair, etc.)")
    args = parser.parse_args()

    # Set up logging.
    if args.outdir:
        ms = 'mip' if args.mip else 'nomip'
        out_dir = f'{args.outdir}/earth_{ms}'
        print (f'Saving results under {out_dir}')
    else:
        out_dir = None
        print ('No output directory specified, not saving log or images')

    # Run.
    fit_earth(object_name = args.object_name, max_iter=args.max_iter, log_interval=10, display_interval=args.display_interval, enable_mip=args.mip, out_dir=out_dir, log_fn='log.txt', texsave_interval=1000, texsave_fn='tex_%06d.png', imgsave_interval=1000, imgsave_fn='img_%06d.png', use_opengl=args.opengl)

    # Done.
    print("Done.")

#----------------------------------------------------------------------------

if __name__ == "__main__":
    main()

#----------------------------------------------------------------------------
